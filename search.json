[
  {
    "objectID": "notebooks/ethena.html",
    "href": "notebooks/ethena.html",
    "title": "TVL",
    "section": "",
    "text": "Code\nimport json, warnings, os\nfrom pathlib import Path\n\nwarnings.filterwarnings('ignore')\n\nif Path.cwd().name == \"notebooks\":\n    rootdir = Path.cwd().parent\nelse:\n    rootdir = Path.cwd()\n\nimport pandas as pd\nimport plotly.express as px\nfrom plotly import graph_objects as go\n\nfrom stables.utils.postgres import get_sqlalchemy_engine\nfrom stables.config import local_pg_config, remote_pg_config\nengine = get_sqlalchemy_engine(local_pg_config)"
  },
  {
    "objectID": "notebooks/ethena.html#usde",
    "href": "notebooks/ethena.html#usde",
    "title": "TVL",
    "section": "USDe",
    "text": "USDe\n\nTVL\n\n\nCode\nquery = \"\"\"\nSELECT * FROM llama.circulating\nWHERE id = 146\n\"\"\"\ndf= pd.read_sql(query, engine)\n\nl = df['time'].max()\n\ntvl = df[(df['time'] == l)]['circulating'].sum()\n\nn_chains = df[\n    (df['time'] == l) \n]['chain'].nunique()\n\n\ndf = df.groupby(['time', \"chain\"]).agg(\n    tvl=('circulating', 'sum')\n).reset_index()\ndf\n\nfig = px.bar(df, x=\"time\", y=\"tvl\", color=\"chain\", title=f\"{l.date()}, TVL: {tvl/1e9:.2f}B\", )\nfig.update_layout(\n    xaxis_title=\"Date\",\n    yaxis_title=\"TVL (USD)\",\n    legend_title_text=\"Chain\"\n)\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nPrimary market\nMint and Redeem\n\n\nCode\nquery = \"\"\"\nWITH combined_events AS (\n    SELECT\n        contract_address,\n        block_timestamp,\n        benefactor,\n        beneficiary,\n        caller,\n        collateral_asset,\n        collateral_amount,\n        event_type,\n        CASE \n            WHEN event_type = 'mint' THEN usde_amount \n            ELSE -usde_amount \n        END as usde_amount\n    FROM ethena_.usde_mint_redeem_v1_events\n\n    UNION ALL\n\n    SELECT \n        contract_address,\n        block_timestamp,\n        benefactor,\n        beneficiary,\n        caller,\n        collateral_asset,\n        collateral_amount,\n        event_type,\n        CASE \n            WHEN event_type = 'mint' THEN usde_amount \n            ELSE -usde_amount \n        END as usde_amount\n    FROM ethena_.usde_mint_redeem_v2_events\n),\n\ndaily_totals AS (\n    SELECT \n        DATE(block_timestamp) as date,\n        event_type,\n        SUM(usde_amount) as usde_amount\n    FROM combined_events\n    GROUP BY DATE(block_timestamp), event_type\n),\n\ndaily_pivot AS (\n    SELECT \n        date,\n        SUM(CASE WHEN event_type = 'mint' THEN usde_amount ELSE 0 END) as mint,\n        SUM(CASE WHEN event_type = 'redeem' THEN usde_amount ELSE 0 END) as redeem,\n        SUM(usde_amount) as net_change\n    FROM daily_totals\n    GROUP BY date\n)\n\nSELECT \n    date,\n    mint / 1e18 as mint,\n    redeem / 1e18 as redeem,\n    net_change / 1e18 as net_change\nFROM daily_pivot\nORDER BY date;\n\"\"\"\ndf= pd.read_sql(query, engine)\n\n# Create the plot\nfig = go.Figure()\n\n# Add mint bars\nfig.add_trace(go.Bar(\n    x=df.index,\n    y=df['mint'],\n    name='Mint',\n    marker_color='green',\n    # opacity=0.7\n))\n\n# Add redeem bars\nfig.add_trace(go.Bar(\n    x=df.index,\n    y=df['redeem'],\n    name='Redeem',\n    marker_color='red',\n    # opacity=0.7\n))\n\n# Add net change line\nfig.add_trace(go.Scatter(\n    x=df.index,\n    y=df['net_change'],\n    name='Net Change',\n    mode='lines',\n    line=dict(color='blue', width=1),\n    # marker=dict(size=2),\n    opacity=0.7\n))\n\n# Update layout\nfig.update_layout(\n    title='Daily USDE Mint/Redeem Activities',\n    xaxis_title='Date',\n    yaxis_title='Amount (USDE)',\n    barmode='group',\n    hovermode='x unified'\n)\n\n# Show the plot\nfig.show() \n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nUSDe staking\n\n\nCode\nquery = \"\"\"\nWITH usde_data AS (\n    SELECT \n        DATE(time) as date,\n        SUM(circulating) as usde\n    FROM llama.circulating\n    WHERE id = 146\n    GROUP BY DATE(time)\n),\nsusde_data AS (\n    SELECT \n        DATE(time) as date,\n        tvl_usd as susde\n    FROM llama.yield_pools\n    WHERE pool_id = '66985a81-9c51-46ca-9977-42b4fe7bc6df'\n),\ncombined_data AS (\n    SELECT \n        COALESCE(u.date, s.date) as date,\n        u.usde,\n        s.susde\n    FROM usde_data u\n    FULL OUTER JOIN susde_data s ON u.date = s.date\n),\nranked_data AS (\n    SELECT \n        date,\n        usde,\n        susde,\n        CASE \n            WHEN usde IS NOT NULL AND usde &gt; 0 \n            THEN (susde / usde) * 100 \n            ELSE NULL \n        END as staking_perc,\n        ROW_NUMBER() OVER (PARTITION BY date ORDER BY date DESC) as rn\n    FROM combined_data\n)\nSELECT \n    date,\n    usde,\n    susde,\n    staking_perc\nFROM ranked_data\nWHERE rn = 1\nORDER BY date\n\"\"\"\ndf = pd.read_sql(query, engine)\ncurrent_staking_perc = df[df['date'] == df['date'].max()]['staking_perc'].values[0]\n\nfig = go.Figure()\nfig.add_trace(go.Bar(x=df['date'], y=df['usde'], name='USDe', opacity=0.5, yaxis='y'))\n\nfig.add_trace(go.Scatter(\n      x=df['date'],\n      y=df['staking_perc'],\n      mode='lines',\n      name='Staking %',\n      yaxis='y2'\n  ))\nfig.update_layout(\n      yaxis2=dict(\n          title=\"%\",\n          overlaying='y',\n          side='right'\n      ),\n    # title=\"Current USDe TVL: {:.2f}B, staking {:.2f}%\".format(tvl_usde/1e9, current_staking_perc),\n    xaxis_title=\"Date\",\n    yaxis_title=\"TVL (USD)\",\n  )\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Stables research",
    "section": "",
    "text": "Dashboard"
  },
  {
    "objectID": "index.html#yield-bearing",
    "href": "index.html#yield-bearing",
    "title": "Stables research",
    "section": "",
    "text": "Dashboard"
  },
  {
    "objectID": "temp/tasks.html",
    "href": "temp/tasks.html",
    "title": "Ethereum Event Log Decoding Task Prompt",
    "section": "",
    "text": "new table\nprompts\n\n\n## Context You need to create a new dbt marts model that decodes Ethereum contract event logs with specific topic filters and data field parsing.\n## Key Information to Gather First 1. Examine existing structure: Look at current marts models and staging models to understand patterns 2. Check available macros: Find hex conversion utilities (hex_to_address, hex_to_numeric, clean_hex_field) 3. Understand data format: Ethereum event logs have: - topic0: Event signature hash - topic1, topic2, topic3: Indexed parameters (32 bytes each, padded) - data: Non-indexed parameters (concatenated 32-byte chunks)\n## Critical Data Decoding Rules 1. Topic decoding: Use hex_to_address('topicN') for address fields 2. Data field structure: - May or may not include 0x prefix (check staging model) - Each parameter is exactly 64 hex characters (32 bytes) - Addresses are right-padded in their 64-char segment 3. Position calculation: - If data has 0x prefix: positions start at 3 (skip “0x”) - Parameter positions: 3-66, 67-130, 131-194, 195-258, etc. - Each parameter is substring(data, start_pos, 64) 4. Address extraction: Use hex_to_address(\"substring(data, pos, 64)\") - the macro handles padding 5. Numeric extraction: Use hex_to_numeric(\"substring(data, pos, 64)\")\n## Validation Approach - Always test with actual example data - Verify each decoded field matches expected output format - Addresses should be 42 chars (0x + 40 hex chars) - Numbers should convert correctly from hex\n## File Structure 1. Create new model in dbt_subprojects/[project]/models/marts/ 2. Update schema.yml with model documentation and tests 3. Follow existing naming conventions and patterns\n## Common Pitfalls - Wrong substring positions (remember 0x prefix affects positioning) - Using wrong macro (hex_to_address vs manual string manipulation) - Not accounting for parameter padding in data field - Forgetting to filter by correct topic0 values\nAlways verify the decoding with real example data before finalizing."
  },
  {
    "objectID": "temp/postgres_setup.html",
    "href": "temp/postgres_setup.html",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "This guide explains how to set up PostgreSQL for the stables data pipeline.\n\n\n\nDocker and Docker Compose (recommended) OR\nPostgreSQL installed locally\n\n\n\n\n\n\nCreate a docker-compose.yml file in the project root:\nversion: '3.8'\n\nservices:\n  postgres:\n    image: postgres:15\n    container_name: stables_postgres\n    environment:\n      POSTGRES_DB: ${POSTGRES_DB}\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    restart: unless-stopped\n\nvolumes:\n  postgres_data:\n\n\n\ndocker-compose up -d\n\n\n\ndocker exec -it stables_postgres psql -U stables_user -d stables -c \"SELECT version();\"\n\n\n\n\n\n\nsudo apt update\nsudo apt install postgresql postgresql-contrib\nsudo systemctl start postgresql\nsudo systemctl enable postgresql\n\n\n\nbrew install postgresql\nbrew services start postgresql\n\n\n\nsudo -u postgres psql\n\nCREATE DATABASE stables;\nCREATE USER stables_user WITH PASSWORD 'your_password_here';\nGRANT ALL PRIVILEGES ON DATABASE stables TO stables_user;\n\\q\n\n\n\n\n\n\nUpdate your .env file with PostgreSQL connection details:\n# PostgreSQL Configuration\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_DB=stables\nPOSTGRES_USER=stables_user\nPOSTGRES_PASSWORD=your_password_here\n\n\n\nuv sync\nThis will install the updated dependencies including: - dbt-postgres - dlt[postgres] - psycopg2-binary\n\n\n\n\nThe PostgreSQL database will use the following schema organization:\n\nraw_curve - Raw Curve Finance data\nraw_ethena - Raw Ethena data\ncrvusd_market - Staged Curve data\nusde - Staged Ethena data\nusde_marts - Ethena marts/analytics tables\n\n\n\n\n\n\npython scripts/curve_dlt_pipeline.py\n\n\n\ncd dbt_subprojects/curve\nuv run dbt run\n\n\n\ndocker exec -it stables_postgres psql -U stables_user -d stables\n\n-- List all schemas\n\\dn\n\n-- List tables in a schema\n\\dt crvusd_market.*\n\n-- Check table contents\nSELECT COUNT(*) FROM crvusd_market.logs;\n\n\n\n\n\n\n\nEnsure PostgreSQL is running: docker-compose ps or sudo systemctl status postgresql\nCheck firewall settings if connecting remotely\nVerify connection details in .env file\n\n\n\n\n\nEnsure the database user has proper permissions\nCheck if the database and schemas exist\n\n\n\n\n\nIncrease PostgreSQL memory settings in postgresql.conf for large datasets\nConsider using connection pooling for high-volume operations\n\n\n\n\n\n\n\nSELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables \nWHERE schemaname NOT IN ('information_schema', 'pg_catalog')\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\n\n\nSELECT count(*) FROM pg_stat_activity;\n\n\n\n\n\n\ndocker exec stables_postgres pg_dump -U stables_user stables &gt; backup.sql\n\n\n\ndocker exec -i stables_postgres psql -U stables_user stables &lt; backup.sql\ndocker exec stables_postgres psql -U postgres -d stables -c \"DROP SCHEMA IF EXISTS yields CASCADE;\"\ndocker exec stables_postgres psql -U postgres -d stables -c \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'yields' ORDER BY table_name;\"\n\n\"SELECT column_name, data_type FROM information_schema.columns WHERE\n      table_schema = 'yields' AND table_name = 'all_pools' AND column_name IN ('reward_tokens', 'underlying_tokens') ORDER BY column_name;\"\n““” sudo lsof -i :5432 docker container ls -a docker rm stables_postgres\ndocker exec stables_postgres psql -U postgres -d stables -c “DROP SCHEMA IF EXISTS usde_ethereum CASCADE;”\n““”"
  },
  {
    "objectID": "temp/postgres_setup.html#prerequisites",
    "href": "temp/postgres_setup.html#prerequisites",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "Docker and Docker Compose (recommended) OR\nPostgreSQL installed locally"
  },
  {
    "objectID": "temp/postgres_setup.html#option-1-docker-setup-recommended",
    "href": "temp/postgres_setup.html#option-1-docker-setup-recommended",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "Create a docker-compose.yml file in the project root:\nversion: '3.8'\n\nservices:\n  postgres:\n    image: postgres:15\n    container_name: stables_postgres\n    environment:\n      POSTGRES_DB: ${POSTGRES_DB}\n      POSTGRES_USER: ${POSTGRES_USER}\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n    restart: unless-stopped\n\nvolumes:\n  postgres_data:\n\n\n\ndocker-compose up -d\n\n\n\ndocker exec -it stables_postgres psql -U stables_user -d stables -c \"SELECT version();\""
  },
  {
    "objectID": "temp/postgres_setup.html#option-2-local-installation",
    "href": "temp/postgres_setup.html#option-2-local-installation",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "sudo apt update\nsudo apt install postgresql postgresql-contrib\nsudo systemctl start postgresql\nsudo systemctl enable postgresql\n\n\n\nbrew install postgresql\nbrew services start postgresql\n\n\n\nsudo -u postgres psql\n\nCREATE DATABASE stables;\nCREATE USER stables_user WITH PASSWORD 'your_password_here';\nGRANT ALL PRIVILEGES ON DATABASE stables TO stables_user;\n\\q"
  },
  {
    "objectID": "temp/postgres_setup.html#configuration",
    "href": "temp/postgres_setup.html#configuration",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "Update your .env file with PostgreSQL connection details:\n# PostgreSQL Configuration\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_DB=stables\nPOSTGRES_USER=stables_user\nPOSTGRES_PASSWORD=your_password_here\n\n\n\nuv sync\nThis will install the updated dependencies including: - dbt-postgres - dlt[postgres] - psycopg2-binary"
  },
  {
    "objectID": "temp/postgres_setup.html#database-schema-structure",
    "href": "temp/postgres_setup.html#database-schema-structure",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "The PostgreSQL database will use the following schema organization:\n\nraw_curve - Raw Curve Finance data\nraw_ethena - Raw Ethena data\ncrvusd_market - Staged Curve data\nusde - Staged Ethena data\nusde_marts - Ethena marts/analytics tables"
  },
  {
    "objectID": "temp/postgres_setup.html#testing-the-setup",
    "href": "temp/postgres_setup.html#testing-the-setup",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "python scripts/curve_dlt_pipeline.py\n\n\n\ncd dbt_subprojects/curve\nuv run dbt run\n\n\n\ndocker exec -it stables_postgres psql -U stables_user -d stables\n\n-- List all schemas\n\\dn\n\n-- List tables in a schema\n\\dt crvusd_market.*\n\n-- Check table contents\nSELECT COUNT(*) FROM crvusd_market.logs;"
  },
  {
    "objectID": "temp/postgres_setup.html#common-issues",
    "href": "temp/postgres_setup.html#common-issues",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "Ensure PostgreSQL is running: docker-compose ps or sudo systemctl status postgresql\nCheck firewall settings if connecting remotely\nVerify connection details in .env file\n\n\n\n\n\nEnsure the database user has proper permissions\nCheck if the database and schemas exist\n\n\n\n\n\nIncrease PostgreSQL memory settings in postgresql.conf for large datasets\nConsider using connection pooling for high-volume operations"
  },
  {
    "objectID": "temp/postgres_setup.html#monitoring",
    "href": "temp/postgres_setup.html#monitoring",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "SELECT \n    schemaname,\n    tablename,\n    pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size\nFROM pg_tables \nWHERE schemaname NOT IN ('information_schema', 'pg_catalog')\nORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;\n\n\n\nSELECT count(*) FROM pg_stat_activity;"
  },
  {
    "objectID": "temp/postgres_setup.html#backup-and-restore",
    "href": "temp/postgres_setup.html#backup-and-restore",
    "title": "PostgreSQL Setup Guide",
    "section": "",
    "text": "docker exec stables_postgres pg_dump -U stables_user stables &gt; backup.sql\n\n\n\ndocker exec -i stables_postgres psql -U stables_user stables &lt; backup.sql\ndocker exec stables_postgres psql -U postgres -d stables -c \"DROP SCHEMA IF EXISTS yields CASCADE;\"\ndocker exec stables_postgres psql -U postgres -d stables -c \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'yields' ORDER BY table_name;\"\n\n\"SELECT column_name, data_type FROM information_schema.columns WHERE\n      table_schema = 'yields' AND table_name = 'all_pools' AND column_name IN ('reward_tokens', 'underlying_tokens') ORDER BY column_name;\"\n““” sudo lsof -i :5432 docker container ls -a docker rm stables_postgres\ndocker exec stables_postgres psql -U postgres -d stables -c “DROP SCHEMA IF EXISTS usde_ethereum CASCADE;”\n““”"
  },
  {
    "objectID": "temp/ethena.html",
    "href": "temp/ethena.html",
    "title": "",
    "section": "",
    "text": "Code\nfrom pathlib import Path\n\nif Path.cwd().name == \"notebooks\" or Path.cwd().name == \"temp\":\n    rootdir = Path.cwd().parent\nelse:\n    rootdir = Path.cwd()\n\n\n\n\nCode\nimport pandas as pd\nfrom stables.utils.postgres import get_sqlalchemy_engine\nfrom stables.config import PostgresConfig\ndb_config = PostgresConfig()\n\nengine = get_sqlalchemy_engine(db_config)\nquery = \"\"\"\nSELECT * FROM ethena_marts.usde_erc20_transfers\n\"\"\"\ndf= pd.read_sql(query, engine)\ncols = ['chainid', 'token_address', 'from_address', 'to_address', 'amount', 'transaction_hash', 'block_timestamp']\n\ndf = df[cols]\ndf.head(5)\n\n\n\n\n\n\n\n\n\nchainid\ntoken_address\nfrom_address\nto_address\namount_hex\namount\nblock_number\nblock_hash\nblock_timestamp\ngas_price\ngas_used\nlog_index\ntransaction_hash\ntransaction_index\n\n\n\n\n0\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n4aced204071b638000\n1379.96204\n18613178\n0x764dda6f921b7b891dd14678220349c967e414a7bccf...\n2023-11-20 13:03:59+00:00\n29470248656\n208883\n140.0\n0xfa74c2ca3efd79b9adcc7ac69cd30be3f6f94f1043c4...\n181.0\n\n\n1\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n4b015541087dc04000\n1383.60186\n18613223\n0x8430db63e4cd5d8b88a526b558adf6ad71430ddf8bb3...\n2023-11-20 13:13:11+00:00\n26270170948\n174683\n153.0\n0x0dc4a16e5a55dc5603a7b75f2476677ab5205f75f889...\n99.0\n\n\n2\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n4b08b2929628b52000\n1384.13253\n18613235\n0x405a6b4fbfc2cc7c1876e0a002e91c60b6f1ff92bae8...\n2023-11-20 13:15:35+00:00\n27625475068\n174683\n241.0\n0xa4a4e7fcc71b656c1fd4cdc4d4a816813d69fb0f8e61...\n81.0\n\n\n3\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n4b1752112760896000\n1385.18623\n18613260\n0x5af1f95de7b6bba8390f6de46fb1b9235bc2f0e00bf7...\n2023-11-20 13:20:35+00:00\n23588296908\n174683\n375.0\n0x3fd897ce32c014c68b762171430620832def551630aa...\n137.0\n\n\n4\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n4b1476eb24e9c2e000\n1384.98043\n18613264\n0xf756900de5eb6d5afe01f56f397850e9768dc0bf674c...\n2023-11-20 13:21:23+00:00\n23021666132\n174683\n436.0\n0xea489b7f9bb936722301c48537572e7df756b47c59d8...\n161.0\n\n\n\n\n\n\n\n\n\nCode\n\ndf\n\n\n\n\n\n\n\n\n\nchainid\ntoken_address\nfrom_address\nto_address\namount\ntransaction_hash\nblock_timestamp\n\n\n\n\n0\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n1379.962040\n0xfa74c2ca3efd79b9adcc7ac69cd30be3f6f94f1043c4...\n2023-11-20 13:03:59+00:00\n\n\n1\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n1383.601860\n0x0dc4a16e5a55dc5603a7b75f2476677ab5205f75f889...\n2023-11-20 13:13:11+00:00\n\n\n2\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n1384.132530\n0xa4a4e7fcc71b656c1fd4cdc4d4a816813d69fb0f8e61...\n2023-11-20 13:15:35+00:00\n\n\n3\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n1385.186230\n0x3fd897ce32c014c68b762171430620832def551630aa...\n2023-11-20 13:20:35+00:00\n\n\n4\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x0000000000000000000000000000000000000000\n0x3fcdebcaf5b31efbe8b10b161ac0fe9a1d359aa5\n1384.980430\n0xea489b7f9bb936722301c48537572e7df756b47c59d8...\n2023-11-20 13:21:23+00:00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n146514\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0xd75f047640a973f8feb6786d9ded055ded637946\n0x1ab3d612ea7df26117554dddd379764ebce1a5ad\n3111.843180\n0xa327222a98f3e996014ae2a08a8855071e3c315eb9e4...\n2024-03-25 07:24:11+00:00\n\n\n146515\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x02950460e2b9529d0e00284a5fa2d7bdf3fa4d72\n0x710bb47414db67482bd3fc86e2463fe400c395de\n8372.917981\n0x49c4376b10a70cd24eaaf0822dca6ac37c8ef01c95e0...\n2024-03-25 07:24:11+00:00\n\n\n146516\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0xc856837ce657dc00ab70f2f8142a8a20b29f2856\n0x9d39a5de30e57443bff2a8307a4256c8797a3497\n10.986471\n0xcb921b391de29a743fec5dd8bf19e920c6d73e69ac4f...\n2024-03-25 07:24:11+00:00\n\n\n146517\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0xe070f56e6d73d63dc671b006c46e1323a89879a1\n0x8707f238936c12c309bfc2b9959c35828acfc512\n10.000000\n0x59265cbd6713e8e2cef7b29c45116e2f437b66c2323a...\n2024-03-25 07:24:23+00:00\n\n\n146518\n1\n0x4c9edd5852cd905f086c759e8383e09bff1e68b3\n0x02950460e2b9529d0e00284a5fa2d7bdf3fa4d72\n0xde5471166523e657d54a50126013daa6daf8dad4\n1497.790572\n0xdc3e22bcf88f66eae496eb961f616d95610408d1891b...\n2024-03-25 07:24:35+00:00\n\n\n\n\n146519 rows × 7 columns\n\n\n\n\n\nCode\n# Create daily aggregation with mint and burn amountsdf['date'] = df['block_timestamp'].dt.datezero_address = '0x0000000000000000000000000000000000000000'# Calculate daily mints (from zero address)daily_mints = df[df['from_address'] == zero_address].groupby('date')['amount'].sum()# Calculate daily burns (to zero address) daily_burns = df[df['to_address'] == zero_address].groupby('date')['amount'].sum()# Calculate total daily transfer volumedaily_total = df.groupby('date')['amount'].sum()# Combine into single dataframedaily_agg = pd.DataFrame({    'mint_amount': daily_mints,    'burn_amount': daily_burns,    'total_amount': daily_total}).fillna(0).round(2).reset_index()print(f\"Daily aggregation shape: {daily_agg.shape}\")daily_agg.head(10)"
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nThis project uses uv for Python package management and PostgreSQL as the data warehouse. To set up the development environment:\nuv sync\n# Set up PostgreSQL (see docs/postgres_setup.md for detailed instructions)\n# Update .env file with PostgreSQL connection details and Etherscan API key\n\n\n\n\n\n\nRun dlt pipeline: python scripts/curve_dlt_pipeline.py - Fetches Curve Finance event logs and loads into PostgreSQL raw_curve schema\nRun dbt transformations:\ncd dbt_subprojects/curve\nuv run dbt run\nThis transforms raw data and saves to PostgreSQL crvusd_market schema\n\n\n\n\n\nInstall dependencies: uv sync\nStart Jupyter: uv run jupyter notebook or uv run jupyter lab\nGenerate website: quarto render (for the Quarto website defined in _quarto.yml)\n\n\n\n\n\nThis is an ELT (Extract, Load, Transform) data pipeline for stablecoin research:\n\nExtract & Load: Python scripts using dlt library fetch blockchain data from Etherscan API and load into PostgreSQL database\nTransform: dbt transforms raw data into analytics-ready tables using SQL\nAnalysis: Jupyter notebooks for data analysis and visualization\n\n\n\n\nscripts/: DLT pipeline scripts for data ingestion (e.g., curve_dlt_pipeline.py for Curve Finance data)\ndbt_subprojects/: DBT projects for data transformation, each with their own dbt_project.yml\nsrc/stables/data/source/: Python modules for different data sources (etherscan, defillama, coingecko)\ndata/address/: Configuration files with contract addresses and pool definitions\ndocs/: Documentation including PostgreSQL setup guide\n\n\n\n\n\nDLT pipelines extract blockchain logs and load into PostgreSQL raw schemas (e.g., raw_curve, raw_ethena)\nDBT models in dbt_subprojects/*/models/ transform raw data\nTransformed data is saved to PostgreSQL staging schemas (e.g., crvusd_market, usde)\nAnalysis notebooks consume both raw and staged data from PostgreSQL\n\n\n\n\n\nPostgreSQL Database: Single database stables with multiple schemas:\n\nraw_curve, raw_ethena - Raw data from DLT pipelines\ncrvusd_market, usde - Staged data from DBT transformations\nusde_marts - Analytics/marts tables\n\nDBT profiles: Each subproject has its own profiles.yml configuration pointing to PostgreSQL\nConnection: Configured via environment variables in .env file\n\n\n\n\n\n\nPython 3.11+ with uv package manager\nDLT (Data Load Tool) for robust data ingestion pipelines\nDBT (Data Build Tool) for SQL-based data transformations\nPostgreSQL as the data warehouse\nJupyter for analysis notebooks\nQuarto for website generation\nEtherscan API as primary blockchain data source\n\n\n\n\nSee docs/postgres_setup.md for detailed instructions on setting up PostgreSQL with Docker or local installation."
  },
  {
    "objectID": "CLAUDE.html#environment-setup",
    "href": "CLAUDE.html#environment-setup",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This project uses uv for Python package management and PostgreSQL as the data warehouse. To set up the development environment:\nuv sync\n# Set up PostgreSQL (see docs/postgres_setup.md for detailed instructions)\n# Update .env file with PostgreSQL connection details and Etherscan API key"
  },
  {
    "objectID": "CLAUDE.html#common-commands",
    "href": "CLAUDE.html#common-commands",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Run dlt pipeline: python scripts/curve_dlt_pipeline.py - Fetches Curve Finance event logs and loads into PostgreSQL raw_curve schema\nRun dbt transformations:\ncd dbt_subprojects/curve\nuv run dbt run\nThis transforms raw data and saves to PostgreSQL crvusd_market schema\n\n\n\n\n\nInstall dependencies: uv sync\nStart Jupyter: uv run jupyter notebook or uv run jupyter lab\nGenerate website: quarto render (for the Quarto website defined in _quarto.yml)"
  },
  {
    "objectID": "CLAUDE.html#architecture-overview",
    "href": "CLAUDE.html#architecture-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This is an ELT (Extract, Load, Transform) data pipeline for stablecoin research:\n\nExtract & Load: Python scripts using dlt library fetch blockchain data from Etherscan API and load into PostgreSQL database\nTransform: dbt transforms raw data into analytics-ready tables using SQL\nAnalysis: Jupyter notebooks for data analysis and visualization\n\n\n\n\nscripts/: DLT pipeline scripts for data ingestion (e.g., curve_dlt_pipeline.py for Curve Finance data)\ndbt_subprojects/: DBT projects for data transformation, each with their own dbt_project.yml\nsrc/stables/data/source/: Python modules for different data sources (etherscan, defillama, coingecko)\ndata/address/: Configuration files with contract addresses and pool definitions\ndocs/: Documentation including PostgreSQL setup guide\n\n\n\n\n\nDLT pipelines extract blockchain logs and load into PostgreSQL raw schemas (e.g., raw_curve, raw_ethena)\nDBT models in dbt_subprojects/*/models/ transform raw data\nTransformed data is saved to PostgreSQL staging schemas (e.g., crvusd_market, usde)\nAnalysis notebooks consume both raw and staged data from PostgreSQL\n\n\n\n\n\nPostgreSQL Database: Single database stables with multiple schemas:\n\nraw_curve, raw_ethena - Raw data from DLT pipelines\ncrvusd_market, usde - Staged data from DBT transformations\nusde_marts - Analytics/marts tables\n\nDBT profiles: Each subproject has its own profiles.yml configuration pointing to PostgreSQL\nConnection: Configured via environment variables in .env file"
  },
  {
    "objectID": "CLAUDE.html#technology-stack",
    "href": "CLAUDE.html#technology-stack",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Python 3.11+ with uv package manager\nDLT (Data Load Tool) for robust data ingestion pipelines\nDBT (Data Build Tool) for SQL-based data transformations\nPostgreSQL as the data warehouse\nJupyter for analysis notebooks\nQuarto for website generation\nEtherscan API as primary blockchain data source"
  },
  {
    "objectID": "CLAUDE.html#postgresql-setup",
    "href": "CLAUDE.html#postgresql-setup",
    "title": "CLAUDE.md",
    "section": "",
    "text": "See docs/postgres_setup.md for detailed instructions on setting up PostgreSQL with Docker or local installation."
  }
]